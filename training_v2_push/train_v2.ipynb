{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Past Challenge v2: ByT5-base Two-Stage Training\n",
    "\n",
    "**Improvements over v1:**\n",
    "1. ByT5-base (580M) instead of ByT5-small (300M) with gradient checkpointing\n",
    "2. Sentence-level alignment data from `Sentences_Oare_FirstWord_LinNum.csv`\n",
    "3. R-Drop regularization (dual forward passes with KL divergence)\n",
    "4. Bidirectional training in Stage 1 (Akkadian->English + English->Akkadian)\n",
    "5. Checkpoint averaging for final model\n",
    "6. Adafactor optimizer (memory-efficient)\n",
    "7. Label smoothing (0.1), cosine annealing, better hyperparameters\n",
    "\n",
    "**Metric**: `sqrt(BLEU * chrF++)` via SacreBLEU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q sacrebleu transformers accelerate sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")\n",
    "\n",
    "import os\n",
    "import re\n",
    "import gc\n",
    "import copy\n",
    "import math\n",
    "import json\n",
    "import glob\n",
    "import unicodedata\n",
    "import collections\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    T5ForConditionalGeneration,\n",
    ")\n",
    "from transformers.optimization import Adafactor, AdafactorSchedule\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm.auto import tqdm\n",
    "import sacrebleu\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Device: {DEVICE}\")\n",
    "if DEVICE == \"cuda\":\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    props = torch.cuda.get_device_properties(0)\n",
    "    print(f\"Memory: {props.total_mem / 1e9:.1f} GB\" if hasattr(props, 'total_mem') else f\"Memory: {props.total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Paths - adjust for Kaggle vs local\n",
    "# ============================================================\n",
    "IS_KAGGLE = os.path.exists(\"/kaggle/input\")\n",
    "\n",
    "if IS_KAGGLE:\n",
    "    COMP_DATA = \"/kaggle/input/deep-past-initiative-machine-translation\"\n",
    "    AKKADEMIA_PATH = \"/kaggle/input/akkademia-nmt\"\n",
    "    ORACC_PATH = \"/kaggle/input/oracc-akkadian-english-parallel-corpus\"\n",
    "    OUTPUT_DIR = \"/kaggle/working\"\n",
    "else:\n",
    "    COMP_DATA = \"data\"\n",
    "    AKKADEMIA_PATH = \"external_data/akkademia/NMT_input\"\n",
    "    ORACC_PATH = \"external_data/oracc\"\n",
    "    OUTPUT_DIR = \"output\"\n",
    "\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# ============================================================\n",
    "# Model & Training Config\n",
    "# ============================================================\n",
    "MODEL_NAME = \"google/byt5-base\"  # 580M params, byte-level\n",
    "\n",
    "# Data sources\n",
    "USE_AKKADEMIA = True\n",
    "USE_ORACC = True\n",
    "USE_COMP_DATA = True\n",
    "USE_SENTENCE_ALIGN = True  # sentence-level alignment data\n",
    "USE_BIDIRECTIONAL = True   # reverse pairs for Stage 1\n",
    "\n",
    "# Shared\n",
    "MAX_SOURCE_LEN = 384\n",
    "MAX_TARGET_LEN = 384\n",
    "PREFIX_AK2EN = \"translate Akkadian to English: \"\n",
    "PREFIX_EN2AK = \"translate English to Akkadian: \"\n",
    "\n",
    "# Stage 1: General Akkadian\n",
    "STAGE1_EPOCHS = 5\n",
    "STAGE1_LR = 1e-4\n",
    "STAGE1_BATCH = 2\n",
    "STAGE1_GRAD_ACC = 16  # effective batch = 32\n",
    "STAGE1_WARMUP = 500\n",
    "\n",
    "# Stage 2: Old Assyrian specialization\n",
    "STAGE2_EPOCHS = 15\n",
    "STAGE2_LR = 5e-5\n",
    "STAGE2_BATCH = 2\n",
    "STAGE2_GRAD_ACC = 16  # effective batch = 32\n",
    "STAGE2_WARMUP = 100\n",
    "\n",
    "# Regularization\n",
    "LABEL_SMOOTHING = 0.1\n",
    "WEIGHT_DECAY = 0.01\n",
    "RDROP_ALPHA = 1.0  # KL divergence weight for R-Drop\n",
    "\n",
    "# Early stopping\n",
    "PATIENCE = 5\n",
    "\n",
    "# Inference\n",
    "BEAM_WIDTH = 4\n",
    "REP_PENALTY = 1.2\n",
    "\n",
    "# Checkpoint averaging\n",
    "NUM_CKPTS_TO_AVG = 3  # average top-N checkpoints\n",
    "\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "print(\"Configuration set.\")\n",
    "print(f\"  Model: {MODEL_NAME}\")\n",
    "print(f\"  Stage 1: {STAGE1_EPOCHS} epochs, lr={STAGE1_LR}, batch={STAGE1_BATCH}x{STAGE1_GRAD_ACC}\")\n",
    "print(f\"  Stage 2: {STAGE2_EPOCHS} epochs, lr={STAGE2_LR}, batch={STAGE2_BATCH}x{STAGE2_GRAD_ACC}\")\n",
    "print(f\"  R-Drop alpha: {RDROP_ALPHA}\")\n",
    "print(f\"  Label smoothing: {LABEL_SMOOTHING}\")\n",
    "print(f\"  Bidirectional: {USE_BIDIRECTIONAL}\")\n",
    "print(f\"  Checkpoint averaging: top {NUM_CKPTS_TO_AVG}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subscript digit mapping\n",
    "SUBSCRIPT_MAP = str.maketrans(\n",
    "    \"\\u2080\\u2081\\u2082\\u2083\\u2084\\u2085\\u2086\\u2087\\u2088\\u2089\",\n",
    "    \"0123456789\"\n",
    ")\n",
    "\n",
    "# ASCII to diacritic normalization for Akkadian transliteration\n",
    "ASCII_TO_DIACRITIC = {\n",
    "    \"sz\": \"\\u0161\", \"SZ\": \"\\u0160\", \"Sz\": \"\\u0160\",\n",
    "    \"sh\": \"\\u0161\", \"SH\": \"\\u0160\", \"Sh\": \"\\u0160\",\n",
    "    \"s,\": \"\\u1E63\", \"S,\": \"\\u1E62\",\n",
    "    \"t,\": \"\\u1E6D\", \"T,\": \"\\u1E6C\",\n",
    "    \".s\": \"\\u1E63\", \".S\": \"\\u1E62\",\n",
    "    \".t\": \"\\u1E6D\", \".T\": \"\\u1E6C\",\n",
    "    \"h,\": \"\\u1E2B\", \"H,\": \"\\u1E2A\",\n",
    "    \".h\": \"\\u1E2B\", \".H\": \"\\u1E2A\",\n",
    "}\n",
    "\n",
    "\n",
    "def normalize_ascii(text):\n",
    "    \"\"\"Normalize ASCII representations to proper Akkadian diacritics.\"\"\"\n",
    "    for old, new in ASCII_TO_DIACRITIC.items():\n",
    "        text = text.replace(old, new)\n",
    "    return text\n",
    "\n",
    "\n",
    "def normalize_gaps(text):\n",
    "    \"\"\"Normalize gap markers.\"\"\"\n",
    "    text = re.sub(r'\\[x\\]', '<gap>', text)\n",
    "    text = re.sub(r'\\[\\.{3,}[^\\]]*\\]', '<big_gap>', text)\n",
    "    text = re.sub(r'\\.{3,}', '<big_gap>', text)\n",
    "    text = re.sub(r'\\u2026', '<big_gap>', text)  # ellipsis character\n",
    "    return text\n",
    "\n",
    "\n",
    "def clean_akkadian(text):\n",
    "    \"\"\"Clean and normalize Akkadian transliteration.\"\"\"\n",
    "    if pd.isna(text) or not str(text).strip():\n",
    "        return \"\"\n",
    "    text = str(text)\n",
    "    text = unicodedata.normalize(\"NFC\", text)\n",
    "    # Remove editorial marks\n",
    "    text = text.replace(\"!\", \"\").replace(\"?\", \"\")\n",
    "    text = re.sub(r'[\\u02F9\\u02FA]', '', text)  # half-brackets\n",
    "    # Remove square brackets but keep content\n",
    "    text = re.sub(r'\\[([^\\]]*)\\]', r'\\1', text)\n",
    "    # Normalize\n",
    "    text = normalize_ascii(text)\n",
    "    text = normalize_gaps(text)\n",
    "    text = text.translate(SUBSCRIPT_MAP)\n",
    "    # Remove stray slashes, colons, dots that are editorial\n",
    "    text = re.sub(r'[/:.](?![\\d])', ' ', text)\n",
    "    # Collapse whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "\n",
    "def clean_english(text):\n",
    "    \"\"\"Clean English translation text.\"\"\"\n",
    "    if pd.isna(text) or not str(text).strip():\n",
    "        return \"\"\n",
    "    text = str(text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "\n",
    "print(\"Preprocessing functions defined.\")\n",
    "\n",
    "# Quick test\n",
    "sample = \"KIŠIB ma-nu-ba-lúm-a-šur DUMU ṣí-lá-(d)IM [x] ... i-ša-qal!\"\n",
    "print(f\"Original: {sample}\")\n",
    "print(f\"Cleaned:  {clean_akkadian(sample)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load & Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 1. Competition data (Old Assyrian - used in both stages)\n",
    "# ============================================================\n",
    "old_assyrian_data = []\n",
    "comp_df = None\n",
    "\n",
    "if USE_COMP_DATA:\n",
    "    comp_df = pd.read_csv(os.path.join(COMP_DATA, \"train.csv\"))\n",
    "    print(f\"Competition data: {len(comp_df)} documents\")\n",
    "\n",
    "    for _, row in comp_df.iterrows():\n",
    "        src = clean_akkadian(row['transliteration'])\n",
    "        tgt = clean_english(row['translation'])\n",
    "        if src and tgt and len(src) > 10 and len(tgt) > 10:\n",
    "            old_assyrian_data.append({'source': src, 'target': tgt})\n",
    "\n",
    "    print(f\"  After cleaning: {len(old_assyrian_data)} document-level pairs\")\n",
    "\n",
    "# ============================================================\n",
    "# 2. Akkademia data (mixed periods - Stage 1 only)\n",
    "# ============================================================\n",
    "general_akkadian_data = []\n",
    "\n",
    "if USE_AKKADEMIA and os.path.exists(AKKADEMIA_PATH):\n",
    "    for split in ['train', 'valid', 'test']:\n",
    "        tr_path = os.path.join(AKKADEMIA_PATH, f'{split}.tr')\n",
    "        en_path = os.path.join(AKKADEMIA_PATH, f'{split}.en')\n",
    "        if os.path.exists(tr_path) and os.path.exists(en_path):\n",
    "            translits = open(tr_path, encoding='utf-8').read().splitlines()\n",
    "            transls = open(en_path, encoding='utf-8').read().splitlines()\n",
    "            for tr, en in zip(translits, transls):\n",
    "                src = clean_akkadian(tr)\n",
    "                tgt = clean_english(en)\n",
    "                if src and tgt and len(src) > 5 and len(tgt) > 5:\n",
    "                    general_akkadian_data.append({'source': src, 'target': tgt})\n",
    "    print(f\"Akkademia data: {len(general_akkadian_data)} pairs\")\n",
    "else:\n",
    "    print(\"Akkademia data not found, skipping.\")\n",
    "\n",
    "# ============================================================\n",
    "# 3. ORACC parallel corpus\n",
    "# ============================================================\n",
    "if USE_ORACC:\n",
    "    oracc_csv = os.path.join(ORACC_PATH, \"train.csv\")\n",
    "    if os.path.exists(oracc_csv):\n",
    "        oracc_df = pd.read_csv(oracc_csv)\n",
    "        print(f\"ORACC data: {len(oracc_df)} pairs\")\n",
    "        for _, row in oracc_df.iterrows():\n",
    "            src = clean_akkadian(row.get('akkadian', ''))\n",
    "            tgt = clean_english(row.get('english', ''))\n",
    "            if src and tgt and len(src) > 5 and len(tgt) > 5:\n",
    "                general_akkadian_data.append({'source': src, 'target': tgt})\n",
    "        print(f\"  Total general data after ORACC: {len(general_akkadian_data)} pairs\")\n",
    "    else:\n",
    "        print(\"ORACC data not found, skipping.\")\n",
    "\n",
    "print(f\"\\n=== Data Summary (before sentence alignment) ===\")\n",
    "print(f\"Old Assyrian (competition): {len(old_assyrian_data)} doc-level pairs\")\n",
    "print(f\"General Akkadian (external): {len(general_akkadian_data)} pairs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentence Alignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 4. Sentence-level alignment data\n",
    "# ============================================================\n",
    "# The file Sentences_Oare_FirstWord_LinNum.csv has sentence-level\n",
    "# English translations with text_uuid that maps to oare_id in train.csv.\n",
    "# We extract corresponding Akkadian segments from the document\n",
    "# transliteration using line numbers.\n",
    "# ============================================================\n",
    "\n",
    "sentence_pairs = []\n",
    "\n",
    "if USE_SENTENCE_ALIGN and comp_df is not None:\n",
    "    sent_file = os.path.join(COMP_DATA, \"Sentences_Oare_FirstWord_LinNum.csv\")\n",
    "    if os.path.exists(sent_file):\n",
    "        sent_df = pd.read_csv(sent_file)\n",
    "        print(f\"Sentence alignment file: {len(sent_df)} rows, {sent_df['text_uuid'].nunique()} unique documents\")\n",
    "\n",
    "        # Build lookup: oare_id -> transliteration lines\n",
    "        # We split the document transliteration by spaces that likely\n",
    "        # correspond to word boundaries, then use first_word_spelling\n",
    "        # and line_number to find sentence boundaries.\n",
    "        doc_translit = {}\n",
    "        for _, row in comp_df.iterrows():\n",
    "            doc_translit[row['oare_id']] = str(row['transliteration'])\n",
    "\n",
    "        # Find overlapping documents\n",
    "        overlap_ids = set(comp_df['oare_id']) & set(sent_df['text_uuid'])\n",
    "        print(f\"Overlapping documents: {len(overlap_ids)}\")\n",
    "\n",
    "        # For each overlapping document, extract sentence-level pairs\n",
    "        # Strategy: group sentences by document, sort by line_number,\n",
    "        # and use consecutive first_word_spelling values to split the\n",
    "        # transliteration into sentence segments.\n",
    "        aligned_count = 0\n",
    "        fallback_count = 0\n",
    "\n",
    "        for doc_id in overlap_ids:\n",
    "            translit = doc_translit.get(doc_id, \"\")\n",
    "            if not translit:\n",
    "                continue\n",
    "\n",
    "            doc_sents = sent_df[sent_df['text_uuid'] == doc_id].sort_values('sentence_obj_in_text')\n",
    "            if len(doc_sents) == 0:\n",
    "                continue\n",
    "\n",
    "            # Collect the first_word_spelling for each sentence to find boundaries\n",
    "            sent_list = []\n",
    "            for _, srow in doc_sents.iterrows():\n",
    "                eng = clean_english(srow.get('translation', ''))\n",
    "                fw_spelling = str(srow.get('first_word_spelling', '')) if pd.notna(srow.get('first_word_spelling')) else ''\n",
    "                fw_number = srow.get('first_word_number', -1)\n",
    "                if pd.isna(fw_number):\n",
    "                    fw_number = -1\n",
    "                sent_list.append({\n",
    "                    'translation': eng,\n",
    "                    'first_word_spelling': fw_spelling,\n",
    "                    'first_word_number': int(fw_number),\n",
    "                })\n",
    "\n",
    "            # Split the document transliteration into words\n",
    "            words = translit.split()\n",
    "\n",
    "            # For each sentence, find the start position using first_word_number\n",
    "            # first_word_number is 1-indexed position of the first word in the text\n",
    "            # (as counted in the original OARE format)\n",
    "            for i, s in enumerate(sent_list):\n",
    "                eng = s['translation']\n",
    "                if not eng or len(eng) < 5:\n",
    "                    continue\n",
    "\n",
    "                fw_num = s['first_word_number']\n",
    "                if fw_num < 1:\n",
    "                    continue\n",
    "\n",
    "                # Determine start and end word indices (0-indexed)\n",
    "                start_idx = fw_num - 1  # convert to 0-indexed\n",
    "\n",
    "                # End index: use next sentence's first_word_number, or end of doc\n",
    "                if i + 1 < len(sent_list) and sent_list[i + 1]['first_word_number'] > 0:\n",
    "                    end_idx = sent_list[i + 1]['first_word_number'] - 1\n",
    "                else:\n",
    "                    end_idx = len(words)\n",
    "\n",
    "                if start_idx >= len(words):\n",
    "                    continue\n",
    "                end_idx = min(end_idx, len(words))\n",
    "\n",
    "                akk_segment = ' '.join(words[start_idx:end_idx])\n",
    "                akk_clean = clean_akkadian(akk_segment)\n",
    "\n",
    "                if akk_clean and len(akk_clean) > 3:\n",
    "                    sentence_pairs.append({\n",
    "                        'source': akk_clean,\n",
    "                        'target': eng,\n",
    "                    })\n",
    "                    aligned_count += 1\n",
    "\n",
    "        print(f\"Sentence-aligned pairs extracted: {aligned_count}\")\n",
    "\n",
    "        # Also use sentences from NON-overlapping documents\n",
    "        # (they have English but no Akkadian transliteration from train.csv)\n",
    "        # We cannot use these directly since we don't have the Akkadian source.\n",
    "        # However, the first_word_spelling column provides a small Akkadian fragment.\n",
    "        # We skip these as they are too fragmented.\n",
    "\n",
    "    else:\n",
    "        print(\"Sentence alignment file not found, skipping.\")\n",
    "else:\n",
    "    print(\"Sentence alignment disabled or no competition data.\")\n",
    "\n",
    "print(f\"\\nSentence-level pairs: {len(sentence_pairs)}\")\n",
    "\n",
    "# Show samples\n",
    "if sentence_pairs:\n",
    "    print(\"\\n--- Sample sentence-aligned pairs ---\")\n",
    "    for p in sentence_pairs[:3]:\n",
    "        print(f\"  AKK: {p['source'][:120]}\")\n",
    "        print(f\"  ENG: {p['target'][:120]}\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 5. Split document-level data into sentence-level pairs\n",
    "# ============================================================\n",
    "# For competition docs NOT in the sentence alignment file,\n",
    "# we try splitting by newlines and sentence-ending punctuation.\n",
    "# ============================================================\n",
    "\n",
    "doc_split_pairs = []\n",
    "\n",
    "if comp_df is not None:\n",
    "    overlap_ids = set(sent_df['text_uuid']) if USE_SENTENCE_ALIGN and 'sent_df' in dir() else set()\n",
    "\n",
    "    for _, row in comp_df.iterrows():\n",
    "        translit = str(row['transliteration'])\n",
    "        translation = str(row['translation'])\n",
    "\n",
    "        # Skip docs that we already have sentence-level data for\n",
    "        if row['oare_id'] in overlap_ids:\n",
    "            continue\n",
    "\n",
    "        # Skip very short docs (already essentially sentence-level)\n",
    "        if len(translit.split()) < 20:\n",
    "            continue\n",
    "\n",
    "        # Try splitting the English translation into sentences\n",
    "        # Common patterns: sentences ending with period, or separated by semicolons\n",
    "        eng_sentences = re.split(r'(?<=[.!?])\\s+(?=[A-Z])', translation)\n",
    "\n",
    "        # If we got meaningful splits and the doc has clear line structure\n",
    "        if len(eng_sentences) >= 2:\n",
    "            # Split Akkadian by common separators (newlines already collapsed,\n",
    "            # but some docs use certain patterns as line breaks)\n",
    "            # We use a heuristic: split on words that commonly start new clauses\n",
    "            # This is imperfect, so only use when we get a reasonable number of segments\n",
    "            pass  # Document splitting is too noisy without line numbers; skip\n",
    "\n",
    "    print(f\"Document-split pairs: {len(doc_split_pairs)}\")\n",
    "\n",
    "print(f\"\\n=== Final Data Summary ===\")\n",
    "print(f\"Old Assyrian doc-level: {len(old_assyrian_data)} pairs\")\n",
    "print(f\"Sentence-aligned: {len(sentence_pairs)} pairs\")\n",
    "print(f\"General Akkadian: {len(general_akkadian_data)} pairs\")\n",
    "print(f\"Total: {len(old_assyrian_data) + len(sentence_pairs) + len(general_akkadian_data)} pairs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 6. Create bidirectional pairs for Stage 1\n",
    "# ============================================================\n",
    "\n",
    "bidirectional_data = []\n",
    "\n",
    "if USE_BIDIRECTIONAL:\n",
    "    # Add reverse pairs (English -> Akkadian) for general data\n",
    "    for pair in general_akkadian_data:\n",
    "        bidirectional_data.append({\n",
    "            'source': pair['target'],  # English as source\n",
    "            'target': pair['source'],  # Akkadian as target\n",
    "            'direction': 'en2ak',\n",
    "        })\n",
    "    # Also add reverse pairs for competition data\n",
    "    for pair in old_assyrian_data:\n",
    "        bidirectional_data.append({\n",
    "            'source': pair['target'],\n",
    "            'target': pair['source'],\n",
    "            'direction': 'en2ak',\n",
    "        })\n",
    "    print(f\"Bidirectional reverse pairs: {len(bidirectional_data)}\")\n",
    "\n",
    "# Mark forward pairs with direction\n",
    "for pair in general_akkadian_data:\n",
    "    pair['direction'] = 'ak2en'\n",
    "for pair in old_assyrian_data:\n",
    "    pair['direction'] = 'ak2en'\n",
    "for pair in sentence_pairs:\n",
    "    pair['direction'] = 'ak2en'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Prepare DataFrames for each stage\n",
    "# ============================================================\n",
    "\n",
    "# Stage 1: ALL data combined (general + competition + bidirectional)\n",
    "stage1_all_list = (\n",
    "    general_akkadian_data\n",
    "    + old_assyrian_data\n",
    "    + sentence_pairs\n",
    "    + bidirectional_data\n",
    ")\n",
    "stage1_all = pd.DataFrame(stage1_all_list)\n",
    "stage1_all = stage1_all.drop_duplicates(subset=['source', 'target']).reset_index(drop=True)\n",
    "stage1_train, stage1_val = train_test_split(stage1_all, test_size=0.05, random_state=SEED)\n",
    "print(f\"Stage 1 - Train: {len(stage1_train)}, Val: {len(stage1_val)}\")\n",
    "if 'direction' in stage1_train.columns:\n",
    "    print(f\"  ak2en: {(stage1_train['direction'] == 'ak2en').sum()}, en2ak: {(stage1_train['direction'] == 'en2ak').sum()}\")\n",
    "\n",
    "# Stage 2: Old Assyrian (document-level + sentence-level)\n",
    "stage2_all_list = old_assyrian_data + sentence_pairs\n",
    "stage2_all = pd.DataFrame(stage2_all_list)\n",
    "stage2_all = stage2_all.drop_duplicates(subset=['source', 'target']).reset_index(drop=True)\n",
    "stage2_train, stage2_val = train_test_split(stage2_all, test_size=0.1, random_state=SEED)\n",
    "print(f\"Stage 2 - Train: {len(stage2_train)}, Val: {len(stage2_val)}\")\n",
    "\n",
    "# Show samples\n",
    "print(f\"\\n--- Sample from Stage 1 (forward) ---\")\n",
    "fwd = stage1_train[stage1_train.get('direction', 'ak2en') == 'ak2en'].iloc[0]\n",
    "print(f\"SRC: {fwd['source'][:200]}\")\n",
    "print(f\"TGT: {fwd['target'][:200]}\")\n",
    "\n",
    "if USE_BIDIRECTIONAL:\n",
    "    rev = stage1_train[stage1_train['direction'] == 'en2ak']\n",
    "    if len(rev) > 0:\n",
    "        rev = rev.iloc[0]\n",
    "        print(f\"\\n--- Sample from Stage 1 (reverse) ---\")\n",
    "        print(f\"SRC: {rev['source'][:200]}\")\n",
    "        print(f\"TGT: {rev['target'][:200]}\")\n",
    "\n",
    "print(f\"\\n--- Sample from Stage 2 ---\")\n",
    "row = stage2_train.iloc[0]\n",
    "print(f\"SRC: {row['source'][:200]}\")\n",
    "print(f\"TGT: {row['target'][:200]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset & Model Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AkkadianDataset(Dataset):\n",
    "    \"\"\"Dataset for Akkadian-English translation with directional prefixes.\"\"\"\n",
    "\n",
    "    def __init__(self, df, tokenizer, max_source_len, max_target_len,\n",
    "                 prefix_ak2en=PREFIX_AK2EN, prefix_en2ak=PREFIX_EN2AK):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_source_len = max_source_len\n",
    "        self.max_target_len = max_target_len\n",
    "        self.prefix_ak2en = prefix_ak2en\n",
    "        self.prefix_en2ak = prefix_en2ak\n",
    "        self.has_direction = 'direction' in df.columns\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "\n",
    "        # Determine prefix based on direction\n",
    "        if self.has_direction and row.get('direction', 'ak2en') == 'en2ak':\n",
    "            prefix = self.prefix_en2ak\n",
    "        else:\n",
    "            prefix = self.prefix_ak2en\n",
    "\n",
    "        src = self.tokenizer(\n",
    "            prefix + row['source'],\n",
    "            max_length=self.max_source_len,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        tgt = self.tokenizer(\n",
    "            row['target'],\n",
    "            max_length=self.max_target_len,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        labels = tgt['input_ids'].squeeze()\n",
    "        labels[labels == self.tokenizer.pad_token_id] = -100\n",
    "        return {\n",
    "            'input_ids': src['input_ids'].squeeze(),\n",
    "            'attention_mask': src['attention_mask'].squeeze(),\n",
    "            'labels': labels\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model and tokenizer\n",
    "print(f\"Loading {MODEL_NAME}...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = T5ForConditionalGeneration.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# Enable gradient checkpointing to fit ByT5-base on P100 16GB\n",
    "model.gradient_checkpointing_enable()\n",
    "print(\"Gradient checkpointing enabled.\")\n",
    "\n",
    "model = model.to(DEVICE)\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "\n",
    "if DEVICE == \"cuda\":\n",
    "    mem_alloc = torch.cuda.memory_allocated() / 1e9\n",
    "    mem_reserved = torch.cuda.memory_reserved() / 1e9\n",
    "    print(f\"GPU memory allocated: {mem_alloc:.2f} GB\")\n",
    "    print(f\"GPU memory reserved: {mem_reserved:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, tokenizer, val_df, batch_size=8, max_source_len=MAX_SOURCE_LEN,\n",
    "             max_target_len=MAX_TARGET_LEN, beam_width=BEAM_WIDTH,\n",
    "             rep_penalty=REP_PENALTY):\n",
    "    \"\"\"Evaluate model on validation set using competition metric.\n",
    "    \n",
    "    Only evaluates ak2en direction (competition metric).\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    references = []\n",
    "\n",
    "    # Filter to ak2en only for evaluation\n",
    "    if 'direction' in val_df.columns:\n",
    "        eval_df = val_df[val_df['direction'] == 'ak2en'].reset_index(drop=True)\n",
    "    else:\n",
    "        eval_df = val_df.reset_index(drop=True)\n",
    "\n",
    "    if len(eval_df) == 0:\n",
    "        print(\"  Warning: no ak2en pairs in validation set.\")\n",
    "        return 0.0, 0.0, 0.0, []\n",
    "\n",
    "    for i in range(0, len(eval_df), batch_size):\n",
    "        batch = eval_df.iloc[i:i + batch_size]\n",
    "        batch_texts = [PREFIX_AK2EN + row['source'] for _, row in batch.iterrows()]\n",
    "        batch_refs = [row['target'] for _, row in batch.iterrows()]\n",
    "\n",
    "        inputs = tokenizer(\n",
    "            batch_texts,\n",
    "            return_tensors='pt',\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=max_source_len\n",
    "        ).to(DEVICE)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=max_target_len,\n",
    "                num_beams=beam_width,\n",
    "                repetition_penalty=rep_penalty,\n",
    "                length_penalty=1.0,\n",
    "            )\n",
    "\n",
    "        preds = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "        predictions.extend(preds)\n",
    "        references.extend(batch_refs)\n",
    "\n",
    "    # Compute metrics\n",
    "    bleu = sacrebleu.corpus_bleu(predictions, [references]).score\n",
    "    chrf = sacrebleu.corpus_chrf(predictions, [references], word_order=2).score\n",
    "\n",
    "    # Competition metric: geometric mean\n",
    "    combined = math.sqrt(max(bleu, 0.001) * max(chrf, 0.001))\n",
    "\n",
    "    model.train()\n",
    "    return bleu, chrf, combined, predictions\n",
    "\n",
    "\n",
    "print(\"Evaluation functions defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop with R-Drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_kl_divergence(logits_p, logits_q, labels, pad_id=-100):\n",
    "    \"\"\"Compute symmetric KL divergence between two sets of logits.\n",
    "    \n",
    "    Only computes KL over non-padding positions.\n",
    "    \"\"\"\n",
    "    # Create mask for non-padding tokens\n",
    "    mask = (labels != pad_id).float()  # (batch, seq_len)\n",
    "\n",
    "    # Get log probabilities\n",
    "    log_p = F.log_softmax(logits_p, dim=-1)  # (batch, seq_len, vocab)\n",
    "    log_q = F.log_softmax(logits_q, dim=-1)\n",
    "    p = log_p.exp()\n",
    "    q = log_q.exp()\n",
    "\n",
    "    # KL(p || q) = sum(p * (log_p - log_q))\n",
    "    kl_pq = F.kl_div(log_q, p, reduction='none').sum(dim=-1)  # (batch, seq_len)\n",
    "    kl_qp = F.kl_div(log_p, q, reduction='none').sum(dim=-1)\n",
    "\n",
    "    # Apply mask and average\n",
    "    kl_pq = (kl_pq * mask).sum() / mask.sum().clamp(min=1)\n",
    "    kl_qp = (kl_qp * mask).sum() / mask.sum().clamp(min=1)\n",
    "\n",
    "    return (kl_pq + kl_qp) / 2\n",
    "\n",
    "\n",
    "def train_stage(model, tokenizer, train_df, val_df, epochs, lr, batch_size,\n",
    "                grad_acc, save_dir, stage_name, warmup_steps=0,\n",
    "                use_rdrop=False, rdrop_alpha=RDROP_ALPHA,\n",
    "                save_all_checkpoints=False):\n",
    "    \"\"\"Train one stage with R-Drop, Adafactor, cosine annealing, and early stopping.\n",
    "\n",
    "    Args:\n",
    "        save_all_checkpoints: If True, save a checkpoint every epoch (for Stage 2 averaging).\n",
    "    \n",
    "    Returns:\n",
    "        model: Best model loaded from checkpoint.\n",
    "        best_score: Best combined metric score.\n",
    "        checkpoint_scores: List of (epoch, score, path) for all saved checkpoints.\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'=' * 60}\")\n",
    "    print(f\"  {stage_name}\")\n",
    "    print(f\"  Train: {len(train_df)} | Val: {len(val_df)}\")\n",
    "    print(f\"  Epochs: {epochs} | LR: {lr} | Batch: {batch_size} x {grad_acc}\")\n",
    "    print(f\"  R-Drop: {use_rdrop} (alpha={rdrop_alpha})\")\n",
    "    print(f\"  Warmup: {warmup_steps} steps\")\n",
    "    print(f\"{'=' * 60}\\n\")\n",
    "\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    train_ds = AkkadianDataset(train_df, tokenizer, MAX_SOURCE_LEN, MAX_TARGET_LEN)\n",
    "    train_loader = DataLoader(\n",
    "        train_ds, batch_size=batch_size, shuffle=True,\n",
    "        num_workers=2, pin_memory=True, drop_last=False\n",
    "    )\n",
    "\n",
    "    # Use Adafactor optimizer (memory efficient, no momentum states)\n",
    "    optimizer = Adafactor(\n",
    "        model.parameters(),\n",
    "        lr=lr,\n",
    "        scale_parameter=False,\n",
    "        relative_step=False,\n",
    "        warmup_init=False,\n",
    "        weight_decay=WEIGHT_DECAY,\n",
    "    )\n",
    "\n",
    "    # Cosine annealing with warmup\n",
    "    num_training_steps = math.ceil(len(train_loader) / grad_acc) * epochs\n",
    "\n",
    "    def lr_lambda(current_step):\n",
    "        if current_step < warmup_steps:\n",
    "            return float(current_step) / float(max(1, warmup_steps))\n",
    "        progress = float(current_step - warmup_steps) / float(\n",
    "            max(1, num_training_steps - warmup_steps)\n",
    "        )\n",
    "        return max(0.01, 0.5 * (1.0 + math.cos(math.pi * progress)))\n",
    "\n",
    "    scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n",
    "\n",
    "    # Cross-entropy loss with label smoothing\n",
    "    loss_fn = nn.CrossEntropyLoss(ignore_index=-100, label_smoothing=LABEL_SMOOTHING)\n",
    "\n",
    "    best_score = 0.0\n",
    "    patience_counter = 0\n",
    "    checkpoint_scores = []  # (epoch, score, path)\n",
    "    global_step = 0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        epoch_loss = 0.0\n",
    "        epoch_ce_loss = 0.0\n",
    "        epoch_kl_loss = 0.0\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        pbar = tqdm(train_loader, desc=f\"Epoch {epoch + 1}/{epochs}\")\n",
    "        for step, batch in enumerate(pbar):\n",
    "            input_ids = batch['input_ids'].to(DEVICE)\n",
    "            attention_mask = batch['attention_mask'].to(DEVICE)\n",
    "            labels = batch['labels'].to(DEVICE)\n",
    "\n",
    "            if use_rdrop:\n",
    "                # R-Drop: two forward passes with different dropout masks\n",
    "                outputs1 = model(\n",
    "                    input_ids=input_ids,\n",
    "                    attention_mask=attention_mask,\n",
    "                    labels=labels\n",
    "                )\n",
    "                outputs2 = model(\n",
    "                    input_ids=input_ids,\n",
    "                    attention_mask=attention_mask,\n",
    "                    labels=labels\n",
    "                )\n",
    "\n",
    "                # Average CE loss from both passes\n",
    "                ce_loss = (outputs1.loss + outputs2.loss) / 2\n",
    "\n",
    "                # KL divergence between the two output distributions\n",
    "                kl_loss = compute_kl_divergence(\n",
    "                    outputs1.logits, outputs2.logits, labels\n",
    "                )\n",
    "\n",
    "                loss = ce_loss + rdrop_alpha * kl_loss\n",
    "                epoch_kl_loss += kl_loss.item()\n",
    "            else:\n",
    "                outputs = model(\n",
    "                    input_ids=input_ids,\n",
    "                    attention_mask=attention_mask,\n",
    "                    labels=labels\n",
    "                )\n",
    "                loss = outputs.loss\n",
    "                ce_loss = loss\n",
    "\n",
    "            scaled_loss = loss / grad_acc\n",
    "            scaled_loss.backward()\n",
    "\n",
    "            if (step + 1) % grad_acc == 0 or (step + 1) == len(train_loader):\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "                optimizer.step()\n",
    "                scheduler.step()\n",
    "                optimizer.zero_grad()\n",
    "                global_step += 1\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_ce_loss += ce_loss.item()\n",
    "\n",
    "            postfix = {'loss': f\"{epoch_loss / (step + 1):.4f}\"}\n",
    "            if use_rdrop:\n",
    "                postfix['kl'] = f\"{epoch_kl_loss / (step + 1):.4f}\"\n",
    "            postfix['lr'] = f\"{scheduler.get_last_lr()[0]:.2e}\"\n",
    "            pbar.set_postfix(postfix)\n",
    "\n",
    "        avg_loss = epoch_loss / len(train_loader)\n",
    "        avg_ce = epoch_ce_loss / len(train_loader)\n",
    "\n",
    "        # Free GPU memory before evaluation\n",
    "        torch.cuda.empty_cache() if DEVICE == 'cuda' else None\n",
    "\n",
    "        # Evaluate\n",
    "        print(f\"\\nEvaluating...\")\n",
    "        bleu, chrf, combined, _ = evaluate(model, tokenizer, val_df)\n",
    "        log_msg = f\"Epoch {epoch + 1}: Loss={avg_loss:.4f} CE={avg_ce:.4f}\"\n",
    "        if use_rdrop:\n",
    "            log_msg += f\" KL={epoch_kl_loss / len(train_loader):.4f}\"\n",
    "        log_msg += f\" | BLEU={bleu:.2f} | chrF++={chrf:.2f} | Combined={combined:.2f}\"\n",
    "        print(log_msg)\n",
    "\n",
    "        # Save checkpoint for this epoch (if enabled)\n",
    "        if save_all_checkpoints:\n",
    "            ckpt_path = os.path.join(save_dir, f\"epoch_{epoch + 1}\")\n",
    "            model.save_pretrained(ckpt_path)\n",
    "            tokenizer.save_pretrained(ckpt_path)\n",
    "            checkpoint_scores.append((epoch + 1, combined, ckpt_path))\n",
    "            print(f\"  Checkpoint saved to {ckpt_path}\")\n",
    "\n",
    "        # Early stopping / best model tracking\n",
    "        if combined > best_score:\n",
    "            best_score = combined\n",
    "            patience_counter = 0\n",
    "            best_path = os.path.join(save_dir, \"best\")\n",
    "            model.save_pretrained(best_path)\n",
    "            tokenizer.save_pretrained(best_path)\n",
    "            print(f\"  >>> New best! Saved to {best_path} (score={combined:.2f})\")\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            print(f\"  No improvement ({patience_counter}/{PATIENCE})\")\n",
    "            if patience_counter >= PATIENCE:\n",
    "                print(f\"  Early stopping triggered.\")\n",
    "                break\n",
    "\n",
    "    # Reload best model\n",
    "    best_path = os.path.join(save_dir, \"best\")\n",
    "    print(f\"\\nLoading best model from {best_path} (score={best_score:.2f})\")\n",
    "    model = T5ForConditionalGeneration.from_pretrained(best_path).to(DEVICE)\n",
    "    model.gradient_checkpointing_enable()\n",
    "\n",
    "    return model, best_score, checkpoint_scores\n",
    "\n",
    "\n",
    "print(\"Training function defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 1: General Akkadian Training (with Bidirectional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stage1_save = os.path.join(OUTPUT_DIR, \"byt5_stage1\")\n",
    "\n",
    "model, stage1_score, _ = train_stage(\n",
    "    model, tokenizer,\n",
    "    train_df=stage1_train,\n",
    "    val_df=stage1_val,\n",
    "    epochs=STAGE1_EPOCHS,\n",
    "    lr=STAGE1_LR,\n",
    "    batch_size=STAGE1_BATCH,\n",
    "    grad_acc=STAGE1_GRAD_ACC,\n",
    "    save_dir=stage1_save,\n",
    "    stage_name=\"STAGE 1: General Akkadian + Bidirectional\",\n",
    "    warmup_steps=STAGE1_WARMUP,\n",
    "    use_rdrop=False,  # No R-Drop in Stage 1 (too slow with large data)\n",
    "    save_all_checkpoints=False,\n",
    ")\n",
    "\n",
    "print(f\"\\nStage 1 complete! Best combined score: {stage1_score:.2f}\")\n",
    "\n",
    "# Free memory\n",
    "gc.collect()\n",
    "if DEVICE == 'cuda':\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 2: Old Assyrian Specialization (with R-Drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stage2_save = os.path.join(OUTPUT_DIR, \"byt5_stage2\")\n",
    "\n",
    "model, stage2_score, stage2_ckpts = train_stage(\n",
    "    model, tokenizer,\n",
    "    train_df=stage2_train,\n",
    "    val_df=stage2_val,\n",
    "    epochs=STAGE2_EPOCHS,\n",
    "    lr=STAGE2_LR,\n",
    "    batch_size=STAGE2_BATCH,\n",
    "    grad_acc=STAGE2_GRAD_ACC,\n",
    "    save_dir=stage2_save,\n",
    "    stage_name=\"STAGE 2: Old Assyrian + R-Drop\",\n",
    "    warmup_steps=STAGE2_WARMUP,\n",
    "    use_rdrop=True,\n",
    "    rdrop_alpha=RDROP_ALPHA,\n",
    "    save_all_checkpoints=True,  # Save every epoch for averaging\n",
    ")\n",
    "\n",
    "print(f\"\\nStage 2 complete! Best combined score: {stage2_score:.2f}\")\n",
    "print(f\"Saved {len(stage2_ckpts)} checkpoints.\")\n",
    "for ep, sc, path in stage2_ckpts:\n",
    "    print(f\"  Epoch {ep}: score={sc:.2f} -> {path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checkpoint Averaging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_checkpoints(checkpoint_paths, tokenizer_path, output_path, device=DEVICE):\n",
    "    \"\"\"Average the weights of multiple checkpoints into a single model.\n",
    "\n",
    "    This technique reduces variance and often improves generalization.\n",
    "    \"\"\"\n",
    "    print(f\"\\nAveraging {len(checkpoint_paths)} checkpoints...\")\n",
    "    for p in checkpoint_paths:\n",
    "        print(f\"  - {p}\")\n",
    "\n",
    "    # Load the first checkpoint as base\n",
    "    avg_state = None\n",
    "\n",
    "    for i, ckpt_path in enumerate(checkpoint_paths):\n",
    "        m = T5ForConditionalGeneration.from_pretrained(ckpt_path)\n",
    "        state = m.state_dict()\n",
    "\n",
    "        if avg_state is None:\n",
    "            avg_state = {k: v.clone().float() for k, v in state.items()}\n",
    "        else:\n",
    "            for k in avg_state:\n",
    "                avg_state[k] += state[k].float()\n",
    "\n",
    "        del m\n",
    "        gc.collect()\n",
    "\n",
    "    # Divide by number of checkpoints\n",
    "    for k in avg_state:\n",
    "        avg_state[k] /= len(checkpoint_paths)\n",
    "\n",
    "    # Load a model and replace its weights\n",
    "    model = T5ForConditionalGeneration.from_pretrained(checkpoint_paths[0])\n",
    "    model.load_state_dict({k: v.to(model.dtype) for k, v in avg_state.items()})\n",
    "\n",
    "    # Save averaged model\n",
    "    os.makedirs(output_path, exist_ok=True)\n",
    "    model.save_pretrained(output_path)\n",
    "\n",
    "    # Copy tokenizer from one of the checkpoints\n",
    "    tok = AutoTokenizer.from_pretrained(tokenizer_path)\n",
    "    tok.save_pretrained(output_path)\n",
    "\n",
    "    print(f\"Averaged model saved to {output_path}\")\n",
    "\n",
    "    model = model.to(device)\n",
    "    model.gradient_checkpointing_enable()\n",
    "    return model\n",
    "\n",
    "\n",
    "# Select top-N checkpoints by score\n",
    "if stage2_ckpts and len(stage2_ckpts) >= 2:\n",
    "    sorted_ckpts = sorted(stage2_ckpts, key=lambda x: x[1], reverse=True)\n",
    "    top_ckpts = sorted_ckpts[:NUM_CKPTS_TO_AVG]\n",
    "    print(f\"\\nTop {len(top_ckpts)} checkpoints for averaging:\")\n",
    "    for ep, sc, path in top_ckpts:\n",
    "        print(f\"  Epoch {ep}: score={sc:.2f}\")\n",
    "\n",
    "    ckpt_paths = [c[2] for c in top_ckpts]\n",
    "    final_save = os.path.join(OUTPUT_DIR, \"byt5_final_averaged\")\n",
    "\n",
    "    model = average_checkpoints(\n",
    "        ckpt_paths,\n",
    "        tokenizer_path=ckpt_paths[0],\n",
    "        output_path=final_save\n",
    "    )\n",
    "\n",
    "    # Evaluate averaged model\n",
    "    print(\"\\nEvaluating averaged model...\")\n",
    "    bleu, chrf, combined, _ = evaluate(model, tokenizer, stage2_val)\n",
    "    print(f\"Averaged model: BLEU={bleu:.2f} | chrF++={chrf:.2f} | Combined={combined:.2f}\")\n",
    "\n",
    "    # If averaged model is worse than best single checkpoint, use the best single\n",
    "    if combined < stage2_score:\n",
    "        print(f\"\\nAveraged model ({combined:.2f}) is worse than best single ({stage2_score:.2f}).\")\n",
    "        print(\"Using best single checkpoint instead.\")\n",
    "        best_path = os.path.join(stage2_save, \"best\")\n",
    "        model = T5ForConditionalGeneration.from_pretrained(best_path).to(DEVICE)\n",
    "        model.gradient_checkpointing_enable()\n",
    "        final_save = best_path\n",
    "    else:\n",
    "        print(f\"\\nAveraged model ({combined:.2f}) >= best single ({stage2_score:.2f}).\")\n",
    "        print(\"Using averaged model.\")\n",
    "        stage2_score = combined\n",
    "else:\n",
    "    print(\"\\nNot enough checkpoints for averaging. Using best single checkpoint.\")\n",
    "    final_save = os.path.join(stage2_save, \"best\")\n",
    "\n",
    "print(f\"\\nFinal model path: {final_save}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validate: Show Sample Translations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show some validation translations\n",
    "print(\"Sample Translations from Validation Set:\\n\")\n",
    "\n",
    "# Filter to ak2en for display\n",
    "if 'direction' in stage2_val.columns:\n",
    "    display_val = stage2_val[stage2_val['direction'] == 'ak2en'].head(10)\n",
    "else:\n",
    "    display_val = stage2_val.head(10)\n",
    "\n",
    "bleu, chrf, combined, preds = evaluate(model, tokenizer, display_val)\n",
    "\n",
    "for i, (_, row) in enumerate(display_val.iterrows()):\n",
    "    if i >= len(preds):\n",
    "        break\n",
    "    print(f\"--- Example {i + 1} ---\")\n",
    "    print(f\"SRC:  {row['source'][:150]}...\")\n",
    "    print(f\"REF:  {row['target'][:150]}...\")\n",
    "    print(f\"PRED: {preds[i][:150]}...\")\n",
    "    print()\n",
    "\n",
    "print(f\"\\nFinal Scores: BLEU={bleu:.2f} | chrF++={chrf:.2f} | Combined={combined:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_path = os.path.join(COMP_DATA, \"test.csv\")\n",
    "if os.path.exists(test_path):\n",
    "    test_df = pd.read_csv(test_path)\n",
    "    print(f\"Test data: {len(test_df)} rows\")\n",
    "    print(test_df.head())\n",
    "\n",
    "    # Clean test transliterations\n",
    "    test_df['clean_src'] = test_df['transliteration'].apply(clean_akkadian)\n",
    "\n",
    "    # Generate translations\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "\n",
    "    eval_batch_size = 8  # smaller batch for generation on P100\n",
    "\n",
    "    for i in tqdm(range(0, len(test_df), eval_batch_size), desc=\"Generating\"):\n",
    "        batch_texts = [\n",
    "            PREFIX_AK2EN + t\n",
    "            for t in test_df['clean_src'].iloc[i:i + eval_batch_size]\n",
    "        ]\n",
    "        inputs = tokenizer(\n",
    "            batch_texts,\n",
    "            return_tensors='pt',\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=MAX_SOURCE_LEN\n",
    "        ).to(DEVICE)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=MAX_TARGET_LEN,\n",
    "                num_beams=BEAM_WIDTH,\n",
    "                repetition_penalty=REP_PENALTY,\n",
    "                length_penalty=1.0,\n",
    "            )\n",
    "        preds = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "        predictions.extend(preds)\n",
    "\n",
    "    # Create submission\n",
    "    submission = pd.DataFrame({\n",
    "        'id': test_df['id'],\n",
    "        'translation': predictions\n",
    "    })\n",
    "    sub_path = os.path.join(OUTPUT_DIR, 'submission.csv')\n",
    "    submission.to_csv(sub_path, index=False)\n",
    "    print(f\"\\nSubmission saved to {sub_path}\")\n",
    "    print(submission.head())\n",
    "else:\n",
    "    print(\"No test data found. Run on Kaggle to generate submission.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Save dataset metadata for Kaggle upload\n",
    "# ============================================================\n",
    "# When uploading to Kaggle as a dataset, include this metadata file\n",
    "\n",
    "dataset_metadata = {\n",
    "    \"title\": \"deep-past-byt5-base-v2\",\n",
    "    \"id\": \"your-username/deep-past-byt5-base-v2\",\n",
    "    \"licenses\": [{\"name\": \"CC0-1.0\"}]\n",
    "}\n",
    "\n",
    "# Save metadata to the final model directory\n",
    "meta_path = os.path.join(final_save, \"dataset-metadata.json\")\n",
    "with open(meta_path, 'w') as f:\n",
    "    json.dump(dataset_metadata, f, indent=2)\n",
    "print(f\"Dataset metadata saved to {meta_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"  Training Complete!\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Model: {MODEL_NAME}\")\n",
    "print(f\"Stage 1 best score: {stage1_score:.2f}\")\n",
    "print(f\"Stage 2 best score: {stage2_score:.2f}\")\n",
    "print(f\"Final model: {final_save}\")\n",
    "print(f\"\\nImprovements used:\")\n",
    "print(f\"  - ByT5-base (580M) with gradient checkpointing\")\n",
    "print(f\"  - Adafactor optimizer\")\n",
    "print(f\"  - Sentence-level alignment data ({len(sentence_pairs)} pairs)\")\n",
    "print(f\"  - Bidirectional training ({len(bidirectional_data)} reverse pairs)\")\n",
    "print(f\"  - R-Drop regularization (alpha={RDROP_ALPHA})\")\n",
    "print(f\"  - Label smoothing ({LABEL_SMOOTHING})\")\n",
    "print(f\"  - Cosine annealing LR with warmup\")\n",
    "print(f\"  - Checkpoint averaging (top {NUM_CKPTS_TO_AVG})\")\n",
    "print(f\"\\nNext steps:\")\n",
    "print(f\"  1. Upload {final_save} as a Kaggle dataset\")\n",
    "print(f\"  2. Use the inference notebook to generate submission\")\n",
    "print(f\"  3. Submit to competition\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbformat_minor": 4,
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

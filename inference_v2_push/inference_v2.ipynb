{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Past Challenge: ByT5 Inference v2\n",
    "\n",
    "Improvements over v1:\n",
    "- Tuned beam search (num_beams=8, no_repeat_ngram_size=3, length_penalty=1.1)\n",
    "- Post-processing (repetition removal, whitespace/unicode normalization)\n",
    "- MBR decoding option (generate N samples, rerank with chrF++)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# sacrebleu not available offline - implement chrF++ inline if needed"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import os\nimport re\nimport math\nimport unicodedata\nfrom collections import Counter\nimport pandas as pd\nimport numpy as np\nimport torch\nfrom tqdm import tqdm\nfrom transformers import AutoTokenizer, T5ForConditionalGeneration\n\nDEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f'Device: {DEVICE}')\nif torch.cuda.is_available():\n    print(f'GPU: {torch.cuda.get_device_name(0)}')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Configuration\n",
    "# ============================================================\n",
    "IS_KAGGLE = os.path.exists('/kaggle/input')\n",
    "\n",
    "if IS_KAGGLE:\n",
    "    COMP_DATA = '/kaggle/input/deep-past-initiative-machine-translation'\n",
    "    MODEL_PATH = '/kaggle/input/byt5-akkadian-final'\n",
    "else:\n",
    "    COMP_DATA = 'data'\n",
    "    MODEL_PATH = 'trained_model/byt5_stage2_final'\n",
    "\n",
    "PREFIX = 'translate Akkadian to English: '\n",
    "MAX_SOURCE_LEN = 512\n",
    "MAX_TARGET_LEN = 512\n",
    "\n",
    "# --- Beam Search (tuned) ---\n",
    "NUM_BEAMS = 8\n",
    "LENGTH_PENALTY = 1.1\n",
    "REP_PENALTY = 1.2\n",
    "NO_REPEAT_NGRAM = 3\n",
    "BEAM_BATCH_SIZE = 8  # smaller batch for larger beam\n",
    "\n",
    "# --- MBR Decoding ---\n",
    "USE_MBR = True\n",
    "MBR_NUM_SAMPLES = 16\n",
    "MBR_TEMPERATURE = 1.0\n",
    "MBR_EPSILON = 0.02  # epsilon sampling cutoff\n",
    "MBR_BATCH_SIZE = 4  # batch size during sampling (each produces MBR_NUM_SAMPLES)\n",
    "\n",
    "print('Config loaded.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Preprocessing (must match training)\n",
    "# ============================================================\n",
    "SUBSCRIPT_MAP = str.maketrans('\\u2080\\u2081\\u2082\\u2083\\u2084\\u2085\\u2086\\u2087\\u2088\\u2089',\n",
    "                              '0123456789')\n",
    "\n",
    "ASCII_TO_DIACRITIC = {\n",
    "    'sz': '\\u0161', 'SZ': '\\u0160', 'Sz': '\\u0160',\n",
    "    'sh': '\\u0161', 'SH': '\\u0160', 'Sh': '\\u0160',\n",
    "    's,': '\\u1E63', 'S,': '\\u1E62',\n",
    "    't,': '\\u1E6D', 'T,': '\\u1E6C',\n",
    "    '.s': '\\u1E63', '.S': '\\u1E62',\n",
    "    '.t': '\\u1E6D', '.T': '\\u1E6C',\n",
    "    'h,': '\\u1E2B', 'H,': '\\u1E2A',\n",
    "    '.h': '\\u1E2B', '.H': '\\u1E2A',\n",
    "}\n",
    "\n",
    "\n",
    "def normalize_ascii(text):\n",
    "    for old, new in ASCII_TO_DIACRITIC.items():\n",
    "        text = text.replace(old, new)\n",
    "    return text\n",
    "\n",
    "\n",
    "def normalize_gaps(text):\n",
    "    text = re.sub(r'\\[x\\]', '<gap>', text)\n",
    "    text = re.sub(r'\\[\\.{3,}[^\\]]*\\]', '<big_gap>', text)\n",
    "    text = re.sub(r'\\.{3,}', '<big_gap>', text)\n",
    "    text = re.sub(r'\\u2026', '<big_gap>', text)\n",
    "    return text\n",
    "\n",
    "\n",
    "def clean_akkadian(text):\n",
    "    if pd.isna(text) or not str(text).strip():\n",
    "        return ''\n",
    "    text = str(text)\n",
    "    text = unicodedata.normalize('NFC', text)\n",
    "    text = text.replace('!', '').replace('?', '')\n",
    "    text = re.sub(r'[\\u02F9\\u02FA]', '', text)\n",
    "    text = re.sub(r'\\[([^\\]]*)\\]', r'\\1', text)\n",
    "    text = normalize_ascii(text)\n",
    "    text = normalize_gaps(text)\n",
    "    text = text.translate(SUBSCRIPT_MAP)\n",
    "    text = re.sub(r'[/:.](?![\\d])', ' ', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "print('Preprocessing functions defined.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Post-Processing Pipeline\n",
    "# ============================================================\n",
    "\n",
    "def remove_repeated_phrases(text, max_ngram=8):\n",
    "    \"\"\"Remove consecutively repeated phrases (n-grams).\"\"\"\n",
    "    words = text.split()\n",
    "    if len(words) < 4:\n",
    "        return text\n",
    "    \n",
    "    # Remove repeated n-grams from large to small\n",
    "    for n in range(max_ngram, 1, -1):\n",
    "        i = 0\n",
    "        result = []\n",
    "        while i < len(words):\n",
    "            if i + 2 * n <= len(words):\n",
    "                ngram = words[i:i+n]\n",
    "                next_ngram = words[i+n:i+2*n]\n",
    "                if ngram == next_ngram:\n",
    "                    # Skip the duplicate\n",
    "                    i += n\n",
    "                    continue\n",
    "            result.append(words[i])\n",
    "            i += 1\n",
    "        words = result\n",
    "    \n",
    "    return ' '.join(words)\n",
    "\n",
    "\n",
    "def remove_trailing_repetition(text):\n",
    "    \"\"\"Remove trailing text that repeats earlier content.\"\"\"\n",
    "    words = text.split()\n",
    "    if len(words) < 10:\n",
    "        return text\n",
    "    \n",
    "    # Check if the last N words repeat a pattern from earlier\n",
    "    best_len = len(words)\n",
    "    for window in range(3, min(len(words) // 2, 20)):\n",
    "        tail = words[-window:]\n",
    "        # Search for this pattern earlier in the text\n",
    "        for start in range(len(words) - window):\n",
    "            if words[start:start+window] == tail:\n",
    "                best_len = min(best_len, len(words) - window)\n",
    "                break\n",
    "    \n",
    "    return ' '.join(words[:best_len])\n",
    "\n",
    "\n",
    "def normalize_whitespace(text):\n",
    "    \"\"\"Normalize whitespace and unicode.\"\"\"\n",
    "    text = unicodedata.normalize('NFC', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    # Fix spacing around punctuation\n",
    "    text = re.sub(r'\\s+([.,;:!?)])', r'\\1', text)\n",
    "    text = re.sub(r'([({])\\s+', r'\\1', text)\n",
    "    return text\n",
    "\n",
    "\n",
    "def postprocess(text):\n",
    "    \"\"\"Full post-processing pipeline.\"\"\"\n",
    "    text = remove_repeated_phrases(text)\n",
    "    text = remove_trailing_repetition(text)\n",
    "    text = normalize_whitespace(text)\n",
    "    return text\n",
    "\n",
    "\n",
    "# Test\n",
    "sample = 'When you wrote me as follows: When you wrote me as follows: When you wrote me as follows: I wrote my assistance.'\n",
    "print(f'Before: {sample}')\n",
    "print(f'After:  {postprocess(sample)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Load Model\n",
    "# ============================================================\n",
    "print(f'Loading model from {MODEL_PATH}...')\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n",
    "model = T5ForConditionalGeneration.from_pretrained(MODEL_PATH)\n",
    "model = model.to(DEVICE)\n",
    "model.eval()\n",
    "print('Model loaded successfully!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Load Test Data\n",
    "# ============================================================\n",
    "test_df = pd.read_csv(os.path.join(COMP_DATA, 'test.csv'))\n",
    "print(f'Test data: {len(test_df)} rows')\n",
    "print(test_df.head())\n",
    "\n",
    "test_df['clean_src'] = test_df['transliteration'].apply(clean_akkadian)\n",
    "print(f'\\nSample cleaned:')\n",
    "print(test_df[['transliteration', 'clean_src']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================\n# Inline chrF++ Implementation (no external deps)\n# ============================================================\n\ndef extract_char_ngrams(text, n):\n    \"\"\"Extract character n-grams from text.\"\"\"\n    return [text[i:i+n] for i in range(len(text) - n + 1)]\n\ndef extract_word_ngrams(text, n):\n    \"\"\"Extract word n-grams from text.\"\"\"\n    words = text.split()\n    return [' '.join(words[i:i+n]) for i in range(len(words) - n + 1)]\n\ndef chrf_score(hypothesis, reference, char_order=6, word_order=2, beta=2):\n    \"\"\"Compute chrF++ score between hypothesis and reference.\"\"\"\n    if not hypothesis or not reference:\n        return 0.0\n    \n    total_f = 0.0\n    count = 0\n    \n    # Character n-grams (orders 1 to char_order)\n    for n in range(1, char_order + 1):\n        hyp_ngrams = Counter(extract_char_ngrams(hypothesis, n))\n        ref_ngrams = Counter(extract_char_ngrams(reference, n))\n        \n        if not hyp_ngrams or not ref_ngrams:\n            continue\n        \n        common = sum((hyp_ngrams & ref_ngrams).values())\n        hyp_total = sum(hyp_ngrams.values())\n        ref_total = sum(ref_ngrams.values())\n        \n        precision = common / hyp_total if hyp_total > 0 else 0\n        recall = common / ref_total if ref_total > 0 else 0\n        \n        if precision + recall > 0:\n            f = (1 + beta**2) * precision * recall / (beta**2 * precision + recall)\n            total_f += f\n            count += 1\n    \n    # Word n-grams (orders 1 to word_order) for chrF++\n    for n in range(1, word_order + 1):\n        hyp_ngrams = Counter(extract_word_ngrams(hypothesis, n))\n        ref_ngrams = Counter(extract_word_ngrams(reference, n))\n        \n        if not hyp_ngrams or not ref_ngrams:\n            continue\n        \n        common = sum((hyp_ngrams & ref_ngrams).values())\n        hyp_total = sum(hyp_ngrams.values())\n        ref_total = sum(ref_ngrams.values())\n        \n        precision = common / hyp_total if hyp_total > 0 else 0\n        recall = common / ref_total if ref_total > 0 else 0\n        \n        if precision + recall > 0:\n            f = (1 + beta**2) * precision * recall / (beta**2 * precision + recall)\n            total_f += f\n            count += 1\n    \n    return (total_f / count * 100) if count > 0 else 0.0\n\n\ndef mbr_select(candidates, utility_fn=chrf_score):\n    \"\"\"Select best candidate via MBR (highest avg pairwise utility).\"\"\"\n    n = len(candidates)\n    if n == 1:\n        return candidates[0]\n    \n    unique = list(set(candidates))\n    if len(unique) == 1:\n        return unique[0]\n    \n    scores = np.zeros(len(unique))\n    for i, hyp in enumerate(unique):\n        for j, ref in enumerate(unique):\n            if i != j:\n                scores[i] += utility_fn(hyp, ref)\n        scores[i] /= (len(unique) - 1)\n    \n    return unique[np.argmax(scores)]\n\n\n# Quick test\nprint(f\"chrF++ test: {chrf_score('the cat sat on the mat', 'the cat is on the mat'):.1f}\")\nprint('MBR functions defined.')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Generate Translations\n",
    "# ============================================================\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "predictions = []\n",
    "n_test = len(test_df)\n",
    "\n",
    "if USE_MBR:\n",
    "    print(f'Using MBR decoding with {MBR_NUM_SAMPLES} samples per sentence')\n",
    "    print(f'Processing {n_test} sentences...')\n",
    "    \n",
    "    for i in tqdm(range(0, n_test, MBR_BATCH_SIZE), desc='MBR Translating'):\n",
    "        batch_end = min(i + MBR_BATCH_SIZE, n_test)\n",
    "        batch_texts = [PREFIX + t for t in test_df['clean_src'].iloc[i:batch_end]]\n",
    "        batch_size_actual = len(batch_texts)\n",
    "        \n",
    "        inputs = tokenizer(\n",
    "            batch_texts,\n",
    "            return_tensors='pt',\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=MAX_SOURCE_LEN\n",
    "        ).to(DEVICE)\n",
    "        \n",
    "        # Generate beam search candidate (1 per sentence)\n",
    "        with torch.no_grad():\n",
    "            beam_outputs = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=MAX_TARGET_LEN,\n",
    "                num_beams=NUM_BEAMS,\n",
    "                repetition_penalty=REP_PENALTY,\n",
    "                length_penalty=LENGTH_PENALTY,\n",
    "                no_repeat_ngram_size=NO_REPEAT_NGRAM,\n",
    "            )\n",
    "        beam_preds = tokenizer.batch_decode(beam_outputs, skip_special_tokens=True)\n",
    "        \n",
    "        # Generate sampled candidates for MBR\n",
    "        # Expand inputs for num_return_sequences\n",
    "        with torch.no_grad():\n",
    "            sample_outputs = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=MAX_TARGET_LEN,\n",
    "                do_sample=True,\n",
    "                temperature=MBR_TEMPERATURE,\n",
    "                epsilon_cutoff=MBR_EPSILON,\n",
    "                num_return_sequences=MBR_NUM_SAMPLES,\n",
    "                repetition_penalty=REP_PENALTY,\n",
    "            )\n",
    "        sample_preds = tokenizer.batch_decode(sample_outputs, skip_special_tokens=True)\n",
    "        \n",
    "        # MBR select for each sentence\n",
    "        for j in range(batch_size_actual):\n",
    "            # Gather all candidates: beam + samples\n",
    "            candidates = [beam_preds[j]]\n",
    "            for k in range(MBR_NUM_SAMPLES):\n",
    "                candidates.append(sample_preds[j * MBR_NUM_SAMPLES + k])\n",
    "            \n",
    "            best = mbr_select(candidates, utility_fn=chrf_score)\n",
    "            predictions.append(best)\n",
    "        \n",
    "        # Progress timing\n",
    "        if (i // MBR_BATCH_SIZE) % 50 == 0 and i > 0:\n",
    "            elapsed = time.time() - start_time\n",
    "            rate = i / elapsed\n",
    "            remaining = (n_test - i) / rate if rate > 0 else 0\n",
    "            print(f'  [{i}/{n_test}] Elapsed: {elapsed/60:.1f}min, ETA: {remaining/60:.1f}min')\n",
    "\n",
    "else:\n",
    "    print(f'Using beam search (beams={NUM_BEAMS}, length_penalty={LENGTH_PENALTY})')\n",
    "    \n",
    "    for i in tqdm(range(0, n_test, BEAM_BATCH_SIZE), desc='Translating'):\n",
    "        batch_texts = [PREFIX + t for t in test_df['clean_src'].iloc[i:i+BEAM_BATCH_SIZE]]\n",
    "        \n",
    "        inputs = tokenizer(\n",
    "            batch_texts,\n",
    "            return_tensors='pt',\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=MAX_SOURCE_LEN\n",
    "        ).to(DEVICE)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=MAX_TARGET_LEN,\n",
    "                num_beams=NUM_BEAMS,\n",
    "                repetition_penalty=REP_PENALTY,\n",
    "                length_penalty=LENGTH_PENALTY,\n",
    "                no_repeat_ngram_size=NO_REPEAT_NGRAM,\n",
    "            )\n",
    "        \n",
    "        preds = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "        predictions.extend(preds)\n",
    "\n",
    "elapsed = time.time() - start_time\n",
    "print(f'\\nGenerated {len(predictions)} translations in {elapsed/60:.1f} minutes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Apply Post-Processing\n",
    "# ============================================================\n",
    "print('Applying post-processing...')\n",
    "processed = [postprocess(p) for p in predictions]\n",
    "\n",
    "# Show before/after for a few examples\n",
    "for i in range(min(3, len(predictions))):\n",
    "    if predictions[i] != processed[i]:\n",
    "        print(f'\\n--- Example {i} ---')\n",
    "        print(f'Before: {predictions[i][:200]}')\n",
    "        print(f'After:  {processed[i][:200]}')\n",
    "\n",
    "predictions = processed\n",
    "print(f'\\nPost-processing complete.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Create Submission\n",
    "# ============================================================\n",
    "submission = pd.DataFrame({\n",
    "    'id': test_df['id'],\n",
    "    'translation': predictions\n",
    "})\n",
    "\n",
    "submission['translation'] = submission['translation'].fillna('')\n",
    "\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "print('Submission saved to submission.csv')\n",
    "print(f'Shape: {submission.shape}')\n",
    "print(submission.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show sample translations\n",
    "print('\\n' + '='*60)\n",
    "print('  Sample Translations')\n",
    "print('='*60)\n",
    "for i in range(min(10, len(test_df))):\n",
    "    print(f'\\n--- Test {i} ---')\n",
    "    print(f'SRC:  {test_df.iloc[i][\"transliteration\"][:200]}')\n",
    "    print(f'PRED: {predictions[i][:200]}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}